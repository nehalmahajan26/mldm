import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(42)  # For reproducibility
X = np.linspace(0, 10, 100)  # 100 points between 0 and 10
y = 2.5 * X + np.random.normal(0, 2, size=X.shape)  # Linear relation with some noise

# Add a column of ones to X for the bias term
X = np.c_[np.ones(X.shape[0]), X]

# Initialize parameters
def initialize_weights(n_features):
    return np.zeros(n_features)

# Define cost function and gradient descent
def compute_cost(X, y, weights):
    predictions = np.dot(X, weights)
    errors = predictions - y
    return np.mean(errors**2)  # Mean Squared Error

def gradient_descent(X, y, weights, learning_rate, epochs):
    m = len(y)
    cost_history = []

    for _ in range(epochs):
        predictions = np.dot(X, weights)
        errors = predictions - y
        gradients = (2/m) * np.dot(X.T, errors)
        weights -= learning_rate * gradients
        cost_history.append(compute_cost(X, y, weights))

    return weights, cost_history

# Train the model
n_features = X.shape[1]
weights = initialize_weights(n_features)
learning_rate = 0.01
epochs = 1000

weights, cost_history = gradient_descent(X, y, weights, learning_rate, epochs)

# Evaluate the model
def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred)**2)

y_pred = np.dot(X, weights)
mse = mean_squared_error(y, y_pred)

print(f"Final Weights: {weights}")
print(f"Mean Squared Error: {mse}")

# Plot linear regression
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 1], y, color='blue', label='Actual Values')
plt.plot(X[:, 1], y_pred, color='red', label='Predicted Values (Linear Regression)')
plt.xlabel('Feature')
plt.ylabel('Target')
plt.title('Linear Regression Fit')
plt.legend()
plt.show()
